1481168203-C-16 is C=16, 1 hidden layer of 600 neurons. Accuracy plateau 0.09

1481170420-C-16: note pitches only (no durations), 600x1 hidden layer, 0.09 accuracy:
> {1: 0.035543483, 2: 0.041050222, 3: 0.041039571, 4: 0.039218191, 5: 0.04322309, 6: 0.047409065, 7: 0.05290515, 8: 0.050764233, 9: 0.061841615, 10: 0.061181232, 11: 0.06125579, 12: 0.074420832, 13: 0.073632635, 14: 0.081173778, 15: 0.080726422, 16: 0.094669007, 17: 0.079970174, 18: 0.095041804, 19: 0.087490015, 20: 0.090898439}

1481170980-C-16: note pitches + durations, 1x800, accuracy capped at .08 but went down a bit:
{1: 0.046620864, 2: 0.041540183, 3: 0.042072747, 4: 0.051382009, 5: 0.04994408, 6: 0.04843159, 7: 0.051989134, 8: 0.061042763, 9: 0.054641318, 10: 0.066751875, 11: 0.070309423, 12: 0.071246736, 13: 0.070991106, 14: 0.080630556, 15: 0.088406026, 16: 0.079970174, 17: 0.081631787, 18: 0.080374926, 19: 0.076433934, 20: 0.086222507}

1481251475-C-8: 1HOT format, context 8, 600x1 hidden layer. 9 epochs: accuracy .94, validation .23

1481252787-C-8: 1HOT format, context 8, 400x1:
training accuracy: {1: 0.31148288942370633, 2: 0.57170680101077975, 3: 0.70412970042500655, 4: 0.78042881309759482, 5: 0.83154837169574691, 6: 0.86642585636242864, 7: 0.88836501665290624, 8: 0.90365019149804449, 9: 0.91629278180112705, 10: 0.92364808392011022}
val accuracy: {1: 0.16682136, 2: 0.20293646, 3: 0.20572104, 4: 0.20732428, 5: 0.21411695, 6: 0.21504514, 7: 0.21601553, 8: 0.2148342, 9: 0.21188085, 10: 0.21732344}

1481253753-C-8: 1HOT format, context 8, 200x1:
train: {1: 0.20948457950490074, 2: 0.37957541215857926, 3: 0.485728769096887, 4: 0.55819391514940708, 5: 0.60844528973480294, 6: 0.64301225263507344, 7: 0.66940642249297133, 8: 0.68932403984782964, 9: 0.70435572414645975, 10: 0.7147190540991204, 11: 0.72514364285765187}
val: {1: 0.17277023, 2: 0.20972914, 3: 0.20896971, 4: 0.20740865, 5: 0.20441313, 6: 0.20403342, 7: 0.2032318, 8: 0.20605856, 9: 0.20217703, 10: 0.20559447, 11: 0.20542571}

same but with 2 layers (100 units each) is garbage: 0.05 acc

1481254388-C-8: 1HOT format, context 8, 100x1:
{1: 0.15042881307685546, 2: 0.27924588046901883, 3: 0.35192860126570907, 4: 0.39950570371592148, 5: 0.42896493474264713, 6: 0.45050274608101981, 7: 0.46533164334765559, 8: 0.47576468040356917, 9: 0.4829066311716278, 10: 0.48830164566692746, 11: 0.49633290855679069, 12: 0.49948035263667695, 13: 0.50382974016726845, 14: 0.5081622279113388, 15: 0.50853189435068646, 16: 0.51309463262784616, 17: 0.51429868805846635, 18: 0.51865441218981723, 19: 0.52004647037753282, 20: 0.52179974479971425} {1: 0.15386887, 2: 0.18694624, 3: 0.18441482, 4: 0.19234663, 5: 0.19622816, 6: 0.19188254, 7: 0.19816893, 8: 0.19618598, 9: 0.19382331, 10: 0.19762045, 11: 0.19365454, 12: 0.19800016, 13: 0.19386549, 14: 0.19373892, 15: 0.1929795, 16: 0.19057463, 17: 0.19378112, 18: 0.19133407, 19: 0.18614464, 20: 0.19458273}

we'll go with 100

1481255187-C-4: 1HOT, 4, 100x1:
train {1: 0.22450757577587296, 2: 0.33849537033926358, 3: 0.38471380470678057, 4: 0.4079650674730238, 5: 0.42347222242993537, 6: 0.43390151581755187, 7: 0.43950336765159259, 8: 0.44647727349791866, 9: 0.44936868676332514, 10: 0.45185606072496887, 11: 0.45377946118212709, 12: 0.4575294613311387, 13: 0.45935185204702195, 14: 0.46228324877794341, 15: 0.46145412446272494, 16: 0.46256313153138062, 17: 0.46538299621280393, 18: 0.46516834982115812, 19: 0.46555765943996835, 20: 0.46672979776154866}
val {1: 0.21739313, 2: 0.22524565, 3: 0.22944486, 4: 0.23759133, 5: 0.22957084, 6: 0.23078862, 7: 0.22637944, 8: 0.22537163, 9: 0.23120853, 10: 0.22944486, 11: 0.22511968, 12: 0.23196439, 13: 0.22646342, 14: 0.2351138, 15: 0.22751322, 16: 0.23112455, 17: 0.22495171, 18: 0.2297388, 19: 0.22957084, 20: 0.22965482}

1481255594-C-4: 1HOT, 4, 50x1:
train {1: 0.18841540416516833, 2: 0.28474116166395069, 3: 0.31630471351584699, 4: 0.33319444436993861, 5: 0.34067340055950962, 6: 0.34731060627735022, 7: 0.35178872083097396, 8: 0.35288510102816301, 9: 0.35468434331694032, 10: 0.35803030305212796, 11: 0.35938762636347249, 12: 0.36043981492820415, 13: 0.36184132968386012, 14: 0.36281355270984195, 15: 0.36380471398282532, 16: 0.36321548827820355, 17: 0.36317971378865871, 18: 0.36346801368500847, 19: 0.36597222234639859, 20: 0.36676557273651011}
val {1: 0.21008651, 2: 0.22579156, 3: 0.23146048, 4: 0.2329722, 5: 0.23326614, 6: 0.22986479, 7: 0.23225833, 8: 0.22894096, 9: 0.22318804, 10: 0.23041068, 11: 0.22755522, 12: 0.23070462, 13: 0.21852691, 14: 0.22663139, 15: 0.22957084, 16: 0.22667338, 17: 0.22633745, 18: 0.2239439, 19: 0.22406988, 20: 0.22902495}

1481255956-C-4: 1HOT, 4, 200x1:
train {1: 0.27789351830612913, 2: 0.43334806411329546, 3: 0.49787036781058169, 4: 0.53823442678108357, 5: 0.56421085766921142, 6: 0.58126893786318379, 7: 0.59580597563674953, 8: 0.60542508353940161, 9: 0.61219907318702849, 10: 0.61989478191191505, 11: 0.62572180122287591, 12: 0.62905723891324472, 13: 0.63253156588685633, 14: 0.6364288720669169, 15: 0.64132786229854888, 16: 0.64300294664472035, 17: 0.64640782871330627, 18: 0.64696128019178756, 19: 0.65023989888905276, 20: 0.65204124717098288}
val {1: 0.21344587, 2: 0.23872513, 3: 0.23964895, 4: 0.23788528, 5: 0.23423196, 6: 0.23880911, 7: 0.23112455, 8: 0.2271353, 9: 0.23410599, 10: 0.22940287, 11: 0.23364407, 12: 0.22755522, 13: 0.22663139, 14: 0.22944486, 15: 0.23351809, 16: 0.23523977, 17: 0.23565969, 18: 0.22684136, 19: 0.22469975, 20: 0.22499371}

1481256577-C-4: 1HOT, 4, 400x1:
train {1: 0.33877104350731407, 2: 0.53792718713256449, 3: 0.61101430891589681, 4: 0.65825968134132296, 5: 0.69183501706580919, 6: 0.71422348495083621, 7: 0.7364036204161668, 8: 0.74893097605819658, 9: 0.7625736523080956, 10: 0.77024831570158103, 11: 0.78102062277571116, 12: 0.78733796340347539, 13: 0.79385521977838847, 14: 0.79867213828997174, 15: 0.80384890678705589, 16: 0.80691287899860231, 17: 0.81135942628889368, 18: 0.81494318046654113, 19: 0.81851641163982525, 20: 0.82064393757268639}
val {1: 0.19740489, 2: 0.22104645, 3: 0.23481986, 4: 0.22936088, 5: 0.24044679, 6: 0.22591753, 7: 0.23490384, 8: 0.22726128, 9: 0.23431595, 10: 0.23083061, 11: 0.22885698, 12: 0.22910893, 13: 0.24032082, 14: 0.2356177, 15: 0.22654741, 16: 0.23519778, 17: 0.23091459, 18: 0.23284623, 19: 0.2319224, 20: 0.22604351}

1481257698-C-2: 1HOT, 2, 25x1:
train {1: 0.18310243525320632, 2: 0.23420864806254804, 3: 0.24189756499129519, 4: 0.24771200669389828, 5: 0.25044080605660035, 6: 0.25052686793828188, 7: 0.25397774978759008, 8: 0.25614819456107668, 9: 0.25751049518209862, 10: 0.25896725399180381, 11: 0.25989504604217989, 12: 0.25962006709812868, 13: 0.26061293001401636, 14: 0.26069269508536275, 15: 0.26065281251215516, 16: 0.26225440795310501, 17: 0.26138329124007775, 18: 0.26284214926494159, 19: 0.26252728806672526, 20: 0.2604408060629812}
val {1: 0.18341014, 2: 0.20104735, 3: 0.19941349, 4: 0.19736071, 5: 0.20624214, 6: 0.20666108, 7: 0.20075409, 8: 0.20963553, 9: 0.20892334, 10: 0.20833683, 11: 0.21537495, 12: 0.20892334, 13: 0.21059908, 14: 0.20938416, 15: 0.2111018, 16: 0.20930038, 17: 0.21424381, 18: 0.21181399, 19: 0.20079598, 20: 0.20967741}

1481257955-C-2: 1HOT, 2, 50x1:
train {1: 0.20675062979436631, 2: 0.26673803493493148, 3: 0.28234886624838601, 4: 0.28962846266420122, 5: 0.29302266966410789, 6: 0.29490973951496768, 7: 0.29777707734734044, 8: 0.30183669132759949, 9: 0.30222921857905927, 10: 0.30203820212324262, 11: 0.30332073818901623, 12: 0.3032850540085914, 13: 0.30374475211017676, 14: 0.30562762352133577, 15: 0.30478169543842226, 16: 0.30603904189818754, 17: 0.30577875669079102, 18: 0.30563811877137165, 19: 0.3055100747259798, 20: 0.30600545697983927}
val {1: 0.19681609, 2: 0.19983242, 3: 0.19501466, 4: 0.2072057, 5: 0.20276497, 6: 0.21022204, 7: 0.2117721, 8: 0.21139506, 9: 0.2098031, 10: 0.21147884, 11: 0.21587767, 12: 0.20695433, 13: 0.20971932, 14: 0.21064097, 15: 0.2131127, 16: 0.20557185, 17: 0.21550062, 18: 0.21315458, 19: 0.20712191, 20: 0.20921659}
